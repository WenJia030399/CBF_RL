Training new model from scratch...
Using cpu device
-----------------------------
| time/              |      |
|    fps             | 49   |
|    iterations      | 1    |
|    time_elapsed    | 20   |
|    total_timesteps | 1024 |
-----------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 52           |
|    iterations           | 2            |
|    time_elapsed         | 39           |
|    total_timesteps      | 2048         |
| train/                  |              |
|    approx_kl            | 0.0062498264 |
|    clip_fraction        | 0.184        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.26        |
|    explained_variance   | -0.0017      |
|    learning_rate        | 0.0005       |
|    loss                 | 924          |
|    n_updates            | 10           |
|    policy_gradient_loss | -0.00147     |
|    std                  | 0.368        |
|    value_loss           | 1.92e+03     |
------------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 47           |
|    iterations           | 3            |
|    time_elapsed         | 65           |
|    total_timesteps      | 3072         |
| train/                  |              |
|    approx_kl            | 0.0052323523 |
|    clip_fraction        | 0.0652       |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.25        |
|    explained_variance   | 0.0016       |
|    learning_rate        | 0.0005       |
|    loss                 | 1.11e+03     |
|    n_updates            | 20           |
|    policy_gradient_loss | -0.00326     |
|    std                  | 0.365        |
|    value_loss           | 2.31e+03     |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 4           |
|    time_elapsed         | 83          |
|    total_timesteps      | 4096        |
| train/                  |             |
|    approx_kl            | 0.011035226 |
|    clip_fraction        | 0.128       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.0322      |
|    learning_rate        | 0.0005      |
|    loss                 | 538         |
|    n_updates            | 30          |
|    policy_gradient_loss | -0.00856    |
|    std                  | 0.365       |
|    value_loss           | 1.14e+03    |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 5           |
|    time_elapsed         | 109         |
|    total_timesteps      | 5120        |
| train/                  |             |
|    approx_kl            | 0.031515222 |
|    clip_fraction        | 0.204       |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.23       |
|    explained_variance   | 0.000116    |
|    learning_rate        | 0.0005      |
|    loss                 | 358         |
|    n_updates            | 40          |
|    policy_gradient_loss | -0.00364    |
|    std                  | 0.365       |
|    value_loss           | 742         |
-----------------------------------------
------------------------------------------
| time/                   |              |
|    fps                  | 48           |
|    iterations           | 6            |
|    time_elapsed         | 127          |
|    total_timesteps      | 6144         |
| train/                  |              |
|    approx_kl            | 0.0048341844 |
|    clip_fraction        | 0.149        |
|    clip_range           | 0.2          |
|    entropy_loss         | -1.24        |
|    explained_variance   | -0.00112     |
|    learning_rate        | 0.0005       |
|    loss                 | 296          |
|    n_updates            | 50           |
|    policy_gradient_loss | -0.00736     |
|    std                  | 0.365        |
|    value_loss           | 617          |
------------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 49          |
|    iterations           | 7           |
|    time_elapsed         | 144         |
|    total_timesteps      | 7168        |
| train/                  |             |
|    approx_kl            | 0.007075878 |
|    clip_fraction        | 0.12        |
|    clip_range           | 0.2         |
|    entropy_loss         | -1.24       |
|    explained_variance   | 0.000317    |
|    learning_rate        | 0.0005      |
|    loss                 | 555         |
|    n_updates            | 60          |
|    policy_gradient_loss | -0.00214    |
|    std                  | 0.366       |
|    value_loss           | 1.13e+03    |
-----------------------------------------
Traceback (most recent call last):
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 111, in <module>
    main()
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 77, in main
    model.learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
    return self.step_wait()
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/wenjia/CBFRL/gazebo_env/env_gazebo.py", line 488, in step
    rclpy.spin_once(self, timeout_sec=self.dt)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/__init__.py", line 206, in spin_once
    executor.spin_once(timeout_sec=timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 739, in spin_once
    self._spin_once_impl(timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 728, in _spin_once_impl
    handler, entity, node = self.wait_for_ready_callbacks(timeout_sec=timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 711, in wait_for_ready_callbacks
    return next(self._cb_iter)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 608, in _wait_for_ready_callbacks
    wait_set.wait(timeout_nsec)
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 111, in <module>
    main()
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 77, in main
    model.learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 218, in collect_rollouts
    new_obs, rewards, dones, infos = env.step(clipped_actions)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py", line 222, in step
    return self.step_wait()
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py", line 59, in step_wait
    obs, self.buf_rews[env_idx], terminated, truncated, self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]
  File "/home/wenjia/CBFRL/gazebo_env/env_gazebo.py", line 488, in step
    rclpy.spin_once(self, timeout_sec=self.dt)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/__init__.py", line 206, in spin_once
    executor.spin_once(timeout_sec=timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 739, in spin_once
    self._spin_once_impl(timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 728, in _spin_once_impl
    handler, entity, node = self.wait_for_ready_callbacks(timeout_sec=timeout_sec)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 711, in wait_for_ready_callbacks
    return next(self._cb_iter)
  File "/opt/ros/humble/local/lib/python3.10/dist-packages/rclpy/executors.py", line 608, in _wait_for_ready_callbacks
    wait_set.wait(timeout_nsec)
KeyboardInterrupt
