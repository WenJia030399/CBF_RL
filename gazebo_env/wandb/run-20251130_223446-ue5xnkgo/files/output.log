Loading pretrained model...
-----------------------------
| time/              |      |
|    fps             | 49   |
|    iterations      | 1    |
|    time_elapsed    | 81   |
|    total_timesteps | 4096 |
-----------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 48          |
|    iterations           | 2           |
|    time_elapsed         | 170         |
|    total_timesteps      | 8192        |
| train/                  |             |
|    approx_kl            | 0.012963405 |
|    clip_fraction        | 0.177       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.08       |
|    explained_variance   | 0.966       |
|    learning_rate        | 0.0005      |
|    loss                 | 7.84        |
|    n_updates            | 220         |
|    policy_gradient_loss | 0.000806    |
|    std                  | 0.94        |
|    value_loss           | 43.3        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 3           |
|    time_elapsed         | 257         |
|    total_timesteps      | 12288       |
| train/                  |             |
|    approx_kl            | 0.014936574 |
|    clip_fraction        | 0.179       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.09       |
|    explained_variance   | 0.724       |
|    learning_rate        | 0.0005      |
|    loss                 | 515         |
|    n_updates            | 230         |
|    policy_gradient_loss | 0.000196    |
|    std                  | 0.95        |
|    value_loss           | 383         |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 4           |
|    time_elapsed         | 354         |
|    total_timesteps      | 16384       |
| train/                  |             |
|    approx_kl            | 0.018755857 |
|    clip_fraction        | 0.18        |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.12       |
|    explained_variance   | 0.889       |
|    learning_rate        | 0.0005      |
|    loss                 | 17.7        |
|    n_updates            | 240         |
|    policy_gradient_loss | 0.00332     |
|    std                  | 0.96        |
|    value_loss           | 51.3        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 5           |
|    time_elapsed         | 434         |
|    total_timesteps      | 20480       |
| train/                  |             |
|    approx_kl            | 0.006896664 |
|    clip_fraction        | 0.0851      |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.676       |
|    learning_rate        | 0.0005      |
|    loss                 | 2.66e+03    |
|    n_updates            | 250         |
|    policy_gradient_loss | -0.00398    |
|    std                  | 0.958       |
|    value_loss           | 849         |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 47          |
|    iterations           | 6           |
|    time_elapsed         | 522         |
|    total_timesteps      | 24576       |
| train/                  |             |
|    approx_kl            | 0.021604402 |
|    clip_fraction        | 0.185       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.13       |
|    explained_variance   | 0.795       |
|    learning_rate        | 0.0005      |
|    loss                 | 9.38        |
|    n_updates            | 260         |
|    policy_gradient_loss | 0.00382     |
|    std                  | 0.959       |
|    value_loss           | 21.1        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 7           |
|    time_elapsed         | 611         |
|    total_timesteps      | 28672       |
| train/                  |             |
|    approx_kl            | 0.008448084 |
|    clip_fraction        | 0.1         |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.14       |
|    explained_variance   | 0.825       |
|    learning_rate        | 0.0005      |
|    loss                 | 21.6        |
|    n_updates            | 270         |
|    policy_gradient_loss | -0.00143    |
|    std                  | 0.963       |
|    value_loss           | 471         |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 8           |
|    time_elapsed         | 699         |
|    total_timesteps      | 32768       |
| train/                  |             |
|    approx_kl            | 0.009387875 |
|    clip_fraction        | 0.113       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.866       |
|    learning_rate        | 0.0005      |
|    loss                 | 10.9        |
|    n_updates            | 280         |
|    policy_gradient_loss | 0.000826    |
|    std                  | 0.968       |
|    value_loss           | 33.5        |
-----------------------------------------
-----------------------------------------
| time/                   |             |
|    fps                  | 46          |
|    iterations           | 9           |
|    time_elapsed         | 795         |
|    total_timesteps      | 36864       |
| train/                  |             |
|    approx_kl            | 0.023898663 |
|    clip_fraction        | 0.141       |
|    clip_range           | 0.2         |
|    entropy_loss         | -4.15       |
|    explained_variance   | 0.749       |
|    learning_rate        | 0.0005      |
|    loss                 | 15.8        |
|    n_updates            | 290         |
|    policy_gradient_loss | -0.00171    |
|    std                  | 0.969       |
|    value_loss           | 477         |
-----------------------------------------
Traceback (most recent call last):
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 111, in <module>
    if __name__ == "__main__":
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 77, in main
    model.learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in collect_rollouts
    rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/buffers.py", line 435, in compute_returns_and_advantage
    self.advantages[step] = last_gae_lam
KeyboardInterrupt
Traceback (most recent call last):
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 111, in <module>
    if __name__ == "__main__":
  File "/home/wenjia/CBFRL/gazebo_env/train.py", line 77, in main
    model.learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py", line 311, in learn
    return super().learn(
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 324, in learn
    continue_training = self.collect_rollouts(self.env, callback, self.rollout_buffer, n_rollout_steps=self.n_steps)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py", line 262, in collect_rollouts
    rollout_buffer.compute_returns_and_advantage(last_values=values, dones=dones)
  File "/home/wenjia/miniconda3/envs/rl/lib/python3.10/site-packages/stable_baselines3/common/buffers.py", line 435, in compute_returns_and_advantage
    self.advantages[step] = last_gae_lam
KeyboardInterrupt
